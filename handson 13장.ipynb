{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"handson 13장","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMfLEV5aAO5wRDffiUH2uFx"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"63HF3aDUa3Lb"},"source":["# 파이썬 ≥3.5 필수\r\n","import sys\r\n","assert sys.version_info >= (3, 5)\r\n","\r\n","# 사이킷런 ≥0.20 필수\r\n","import sklearn\r\n","assert sklearn.__version__ >= \"0.20\"\r\n","\r\n","try:\r\n","    # %tensorflow_version은 코랩 명령입니다.\r\n","    %tensorflow_version 2.x\r\n","    !pip install -q -U tfx\r\n","    print(\"패키지 호환 에러는 무시해도 괜찮습니다.\")\r\n","except Exception:\r\n","    pass\r\n","\r\n","# 텐서플로 ≥2.0 필수\r\n","import tensorflow as tf\r\n","from tensorflow import keras\r\n","assert tf.__version__ >= \"2.0\"\r\n","\r\n","# 공통 모듈 임포트\r\n","import numpy as np\r\n","import os\r\n","\r\n","# 노트북 실행 결과를 동일하게 유지하기 위해\r\n","np.random.seed(42)\r\n","\r\n","# 깔끔한 그래프 출력을 위해\r\n","%matplotlib inline\r\n","import matplotlib as mpl\r\n","import matplotlib.pyplot as plt\r\n","mpl.rc('axes', labelsize=14)\r\n","mpl.rc('xtick', labelsize=12)\r\n","mpl.rc('ytick', labelsize=12)\r\n","\r\n","# 그림을 저장할 위치\r\n","PROJECT_ROOT_DIR = \".\"\r\n","CHAPTER_ID = \"data\"\r\n","IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\r\n","os.makedirs(IMAGES_PATH, exist_ok=True)\r\n","\r\n","def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\r\n","    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\r\n","    print(\"그림 저장:\", fig_id)\r\n","    if tight_layout:\r\n","        plt.tight_layout()\r\n","    plt.savefig(path, format=fig_extension, dpi=resolution)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w7M9xq167fwX"},"source":["# 13.1 데이터 API"]},{"cell_type":"code","metadata":{"id":"4QoIfWmjsecy"},"source":["X = tf.range(10)\r\n","\r\n","for i in X:\r\n","  print(X)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G71T5rzT8SuW"},"source":["from_tensor_slices()\r\n","\r\n","X의 각 원소가 아이템으로 표현됨\r\n"]},{"cell_type":"code","metadata":{"id":"SQ-uUVIL8D-8"},"source":["X= tf.range(10)\r\n","dataset = tf.data.Dataset.from_tensor_slices(X)\r\n","\r\n","for i in dataset:\r\n","  print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NU8Pkes_8eiX"},"source":["### 13.1.1 연쇄 변환"]},{"cell_type":"code","metadata":{"id":"UBRHByHu8NR1"},"source":["dataset = dataset.repeat(3).batch(7)\r\n","#세번 반복되는 데이터셋으로 바꾼 후 7개씩 묶는 연쇄변환"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_0gn1As88jM"},"source":["for i in dataset:\r\n","  print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8X7FIEBB9V3R"},"source":["map 가능 : 각 아이템에 변환"]},{"cell_type":"code","metadata":{"id":"-OC7ERjy8_2k"},"source":["dataset = dataset.map(lambda x : x+2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LzEGe_6a9aty"},"source":["for i in dataset:\r\n","  print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-JWz7UDG92qG"},"source":["apply 가능: 전체 데이터셋을 변환"]},{"cell_type":"code","metadata":{"id":"9xqBZXx09dX2"},"source":["dataset = dataset.apply(tf.data.Dataset.unbatch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7LGl3b798tm"},"source":["for i in dataset:\r\n","  print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hS_TSczv-H2s"},"source":["dataset = dataset.filter(lambda x: x<10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0ozCUGI-S-G"},"source":["for i in dataset:\r\n","  print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRLHkzd--aMo"},"source":["### 13.1.2 데이터 셔플링"]},{"cell_type":"code","metadata":{"id":"TllrBRac-XlE"},"source":["dataset = tf.data.Dataset.range(10).repeat(3)\r\n","dataset = dataset.shuffle(buffer_size=5,seed=42).batch(7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ofIkjSVg_J_9"},"source":["for i in dataset:\r\n","  print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9dFVV9v4NRBq"},"source":["여러 파일에서 한줄씩 번갈아 읽기 (진짜 셔플을 위해)"]},{"cell_type":"code","metadata":{"id":"WosQ768d_LV0"},"source":["from sklearn.datasets import fetch_california_housing\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.preprocessing import StandardScaler\r\n","\r\n","housing = fetch_california_housing()\r\n","X_train_full, X_test, y_train_full, y_test = train_test_split(\r\n","    housing.data, housing.target.reshape(-1,1), random_state=42    \r\n",")\r\n","\r\n","X_train,X_valid, y_train,y_valid = train_test_split(\r\n","    X_train_full,y_train_full,random_state=42\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXUwzP1ZN1ow"},"source":["scaler = StandardScaler()\r\n","scaler.fit(X_train)\r\n","X_mean = scaler.mean_\r\n","X_std = scaler.scale_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"edpN2DD6PgNP"},"source":["메모리에 맞지 않는 매우 큰 데이터셋인 경우 일반적으로 먼저 여러 개의 파일로 나누고 텐서플로에서 이 파일들을 병렬로 읽게함.\r\n","\r\n"," 데모를 위해 주택 데이터셋을 20개의 CSV 파일 분할"]},{"cell_type":"code","metadata":{"id":"95DGfaMvOIhP"},"source":["def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\r\n","    housing_dir = os.path.join(\"datasets\", \"housing\")\r\n","    os.makedirs(housing_dir, exist_ok=True)\r\n","    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\r\n","\r\n","    filepaths = []\r\n","    m = len(data)\r\n","    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\r\n","        part_csv = path_format.format(name_prefix, file_idx)\r\n","        filepaths.append(part_csv)\r\n","        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\r\n","            if header is not None:\r\n","                f.write(header)\r\n","                f.write(\"\\n\")\r\n","            for row_idx in row_indices:\r\n","                f.write(\",\".join([repr(col) for col in data[row_idx]]))\r\n","                f.write(\"\\n\")\r\n","    return filepaths"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lx7UDVEDPKEi"},"source":["train_data = np.c_[X_train, y_train]\r\n","valid_data = np.c_[X_valid, y_valid]\r\n","test_data = np.c_[X_test, y_test]\r\n","header_cols = housing.feature_names + [\"MedianHouseValue\"]\r\n","header = \",\".join(header_cols)\r\n","\r\n","train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\r\n","valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\r\n","test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGrIX7OmPL6i"},"source":["import pandas as pd\r\n","pd.read_csv(train_filepaths[0]).head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F5GvT_Y7PXkY"},"source":["with open(train_filepaths[0]) as f:\r\n","  for i in range(5):\r\n","    print(f.readline(), end=\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jvMe_wiMQfP6"},"source":["train_filepaths"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I1BtPfLAQ4FB"},"source":["입력 파이프라인 만들기"]},{"cell_type":"markdown","metadata":{"id":"i5AECD_kSGfn"},"source":["list_files()"]},{"cell_type":"code","metadata":{"id":"vae1LvJjQ2KQ"},"source":["filepath_dataset = tf.data.Dataset.list_files(train_filepaths,seed=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rbgeVNhYRnMH"},"source":["#호출할때마다 섞어서 반환됨\r\n","for filepath in filepath_dataset:\r\n","  print(filepath)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_WfYwtikSLlF"},"source":["interleave()"]},{"cell_type":"code","metadata":{"id":"M8ZXWq08RumK"},"source":["n_readers= 5\r\n","\r\n","#interleave는 filepath_dataset에 있는 파일 경로 다섯개를 번갈아가면서 읽음.\r\n","dataset = filepath_dataset.interleave(\r\n","    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\r\n","    cycle_length = n_readers\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kr2fvvviSiCX"},"source":["for line in dataset.take(5):\r\n","  print(line.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J_8RHy-9UxRq"},"source":["### 13.1.3 데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"tC-w2whWQafX"},"source":["tf.io.decode_csv 의 디폴트값 지정 방법"]},{"cell_type":"code","metadata":{"id":"X5PXzUMpO35O"},"source":["#디폴트값 지정\n","record_defaults=[0, np.nan, tf.constant(np.nan, dtype=tf.float64), \"Hello\", tf.constant([])]\n","#1,2,3,4,5를 각 특성이 어떻게 해석하는지\n","parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\n","\n","parsed_fields"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t8mBgOjSPP7F"},"source":["#, , , , 5를 어떻게 해석하는지\n","parsed_fields = tf.io.decode_csv(',,,,5', record_defaults)\n","parsed_fields"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCzZu7_QPu68"},"source":["#다른 값들과 다르게 다섯번째 필드는 기본값을 지정하지 않음 --> tf.constant([])\n","#따라서 ,,,,로값을 전달하지 않으면 예외가 발생함. \n","try:\n","    parsed_fields = tf.io.decode_csv(',,,,', record_defaults)\n","except tf.errors.InvalidArgumentError as ex:\n","    print(ex)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uCdAVhhzRDu9"},"source":["#필드 개수는 record_defaults의 필드 수와도 맞아야함.\n","try:\n","    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\n","except tf.errors.InvalidArgumentError as ex:\n","    print(ex)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cP4HOnegR4Hg"},"source":["전처리를 위한 간단한 함수 만들어보기"]},{"cell_type":"code","metadata":{"id":"vYB7MIzJURYn"},"source":["n_inputs = 8 \n","\n","def preprocess(line):\n","  defs = [0.] * n_inputs +[tf.constant([], dtype = tf.float32)]\n","  fields = tf.io.decode_csv(line, record_defaults=defs)\n","  x= tf.stack(fields[:-1])\n","  y =tf.stack(fields[-1:])\n","  return (x-X_mean)/X_std , y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pp2b37yCTRhm"},"source":["preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"czcA2HF-UPQU"},"source":["### 13.1.4 데이터 적재와 전처리를 합치기"]},{"cell_type":"markdown","metadata":{"id":"uGY8yzzzUXfU"},"source":["앞서 배운 적재, 전처리, 셔플링, 반복, 배치를 적용"]},{"cell_type":"code","metadata":{"id":"TCWGi07JTeQu"},"source":["def csv_reader_dataset(filepaths, repeat=1,n_readers=5,\n","                       n_read_threads=None, shuffle_buffer_size =10000,\n","                       n_parse_threads=5, batch_size=32):\n","  dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n","  #파일경로를 섞은 데이터셋 반환\n","  dataset = dataset.interleave(\n","      lambda filepath : tf.data.TextLineDataset(filepath).skip(1),\n","      cycle_length = n_readers, num_parallel_calls = n_read_threads\n","  )\n","  #번갈아가며 읽기. \n","  #cycle_length 몇개파일을 번갈아가면 읽을지 , num_parallel_calls 병렬처리\n","  \n","  dataset = dataset.shuffle(shuffle_buffer_size)\n","  #interleave 후 shuffle --> 완벽? 한 셔플\n","  dataset = dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n","  dataset = dataset.batch(batch_size)\n","  return dataset.prefetch(1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bA8H62kdWX6j"},"source":["tf.random.set_seed(42)\n","\n","train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n","for X_batch,y_batch in train_set.take(2):\n","  print(\"X = \",X_batch)\n","  print(\"y = \",y_batch)\n","  print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_nmfQljbbIUm"},"source":["### 13.1.6 tf.keras와 데이터셋 사용하기"]},{"cell_type":"code","metadata":{"id":"F7CtQCdgaHbQ"},"source":["train_set = csv_reader_dataset(train_filepaths, repeat=None) #데이터셋 무한반복\n","valid_set = csv_reader_dataset(valid_filepaths)\n","test_set = csv_reader_dataset(test_filepaths)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N12xjyKlbt41"},"source":["model = keras.models.Sequential([\n","                                keras.layers.Dense(30,activation='relu',\n","                                                   input_shape=X_train.shape[1:]),\n","                                keras.layers.Dense(1)\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lhKZ6QgGb_Mh"},"source":["model.compile(loss= keras.losses.MeanSquaredError(), optimizer = keras.optimizers.SGD(lr=1e-3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Bhujy4ScP2T"},"source":["batch_size=32\n","model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n","          validation_data=valid_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tBAdNE2PfqmX"},"source":["evaluate 가능"]},{"cell_type":"code","metadata":{"id":"IqBnjKMactSI"},"source":["model.evaluate(test_set, steps = len(X_test)//batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qyn1U4iLfoOZ"},"source":["predict 가눙~"]},{"cell_type":"code","metadata":{"id":"pqLcfFdted2p"},"source":["new_set = test_set.map(lambda X,y : X) #레이블 없애기\n","X_new = X_test\n","model.predict(new_set, steps= len(X_new)//batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i56hd3hzflJG"},"source":["사용자 정의 훈련반복도 가능~"]},{"cell_type":"code","metadata":{"id":"jONhMkQhfkKo"},"source":["n_epochs=5\n","batch_size=32\n","n_steps_per_epochs = len(X_train)//batch_size\n","total_steps = n_epochs * n_steps_per_epochs\n","#train_set 이 지금 무한 repeat이기 때문에 끝을 직접 계산해서 정해줘야함\n","global_steps =0\n","\n","optimizer = keras.optimizers.Nadam(lr=0.01)\n","loss_fn = keras.losses.mean_squared_error\n","\n","\n","\n","\n","for X_batch, y_batch in train_set.take(total_steps):\n","  global_steps +=1\n","  print(\"\\rGlobal step {}/{}\".format(global_steps,total_steps), end = \"\")\n","\n","  with tf.GradientTape() as tape:\n","    y_pred = model(X_batch)\n","    main_loss = tf.reduce_mean(loss_fn(y_pred,y_batch))\n","    loss = tf.add_n([main_loss] + model.losses)\n","  gradient = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"48Wq8r7Wjr74"},"source":["전체 훈련을 반복하는 텐서플로 함수 만들어보기"]},{"cell_type":"code","metadata":{"id":"8K9VZOrrfV7-"},"source":["optimizer = keras.optimizers.Nadam(lr = 0.01)\n","loss_fn = keras.losses.mean_squared_error\n","\n","@tf.function\n","def train(model, n_epochs, batch_size= 32,\n","          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000,\n","          n_parse_threads=5):\n","  train_set =csv_reader_dataset(train_filepaths,repeat=n_epochs, \n","                                n_readers=n_readers,\n","                                n_read_threads = n_read_threads, \n","                                shuffle_buffer_size=shuffle_buffer_size,\n","                                n_parse_threads= n_parse_threads, \n","                                batch_size=batch_size)\n","  global_steps=0\n","  for X_batch,y_batch in train_set:\n","    global_steps+=1\n","    if tf.equal(global_steps % 100,0):\n","      tf.print(\"\\rGlobal step\", global_steps, \"/\", total_steps)\n","    with tf.GradientTape() as tape:\n","      y_pred = model(X_batch)\n","      main_loss = tf.reduce_mean(loss_fn(y_pred,y_batch))\n","      loss = tf.add_n([main_loss] + model.losses)\n","\n","    gradient = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradient,model.trainable_variables))\n","\n","train(model,n_epochs=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iwT9chqIfhhs"},"source":["keras.backend.clear_session()\n","np.random.seed(42)\n","tf.random.set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P8mKGO87mQ87"},"source":["# 13.2 TFRecord 포맷"]},{"cell_type":"markdown","metadata":{"id":"xz2z_hrLmt4Y"},"source":["크기가 다른 연속된 이진 레코드를 저장하는 단순한 이진포멧"]},{"cell_type":"code","metadata":{"id":"_CQ5sjm8lq_-"},"source":["with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n","  f.write(b\"this is the first record\")\n","  f.write(b\"and this is the second record\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k1U7TJPbm9KR"},"source":["filepaths = [\"my_data.tfrecord\"]\n","datset =tf.data.TFRecordDataset(filepaths)\n","for item in datset:\n","  print(item)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9o7cuSI1nVp0"},"source":["### 13.2.1 압축된 TFRecord 파일"]},{"cell_type":"code","metadata":{"id":"XCyIY-IPnI9-"},"source":["#options 매개변수를 만들어서 압축가능.\n","options = tf.io.TFRecordOptions(compression_type = \"GZIP\")\n","with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n","  f.write(b\"This is the first record\")\n","  f.write(b\"And this is the second record\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aGR3mqhwt34l"},"source":["dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n","                                  compression_type=\"GZIP\")\n","\n","for item in dataset:\n","  print(item)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gn4IHvy3uXva"},"source":["### 13.2.2 프로토콜 버퍼 개요"]},{"cell_type":"markdown","metadata":{"id":"xxncbbx9vBKi"},"source":["각 숫자1,2,3은 필드식별자"]},{"cell_type":"code","metadata":{"id":"L9fPkW03uFXV"},"source":["%%writefile person.proto\n","syntax = \"proto3\";\n","message Person {\n","  string name = 1;\n","  int32 id = 2;\n","  repeated string email = 3;\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7WZaJHa2uoa2"},"source":["!protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KG-n307HusWn"},"source":["!ls person*"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PYA6RbcSutam"},"source":["from person_pb2 import Person\n","\n","person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # Person 생성\n","print(person)  # Person 출력"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVG8PTNYuzL3"},"source":["person.name #필드읽기"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJZhswuCu18s"},"source":["person.name = \"Alice\" #필드 수정하기"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1g5WLfSyu3TN"},"source":["person.email[0] #배열처럼 사용하는 반복필드"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GxYATmk6u5AL"},"source":["person.email.append(\"c@d.com\") #반복필드 추가"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6N_TNEKDvBaL"},"source":["바이트 문자열로 직렬화"]},{"cell_type":"code","metadata":{"id":"pxxUL84ru81m"},"source":["s = person.SerializeToString()\n","s"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T3VM9FB0vLno"},"source":["직렬화한 바이트 문자열 파싱하기"]},{"cell_type":"code","metadata":{"id":"rQvotxTxvC4S"},"source":["person2 =Person()\n","person2.ParseFromString(s)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vsl_sNB7vKKu"},"source":["person == person2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jTzGLYdQwkwt"},"source":["tf.io.decode_proto()\n","\n","(앞서 우리가 만든 것처럼) 사용자 정의 프로토콜 버퍼를 파싱해야 할때 사용"]},{"cell_type":"code","metadata":{"id":"NKpLEWRfwoYX"},"source":["person_tf = tf.io.decode_proto(\n","    bytes=s,\n","    message_type=\"Person\",\n","    field_names=[\"name\", \"id\", \"email\"],\n","    output_types=[tf.string, tf.int32, tf.string],\n","    descriptor_source=\"person.desc\")\n","\n","person_tf.values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KbtnDKxFwKGh"},"source":["### 12.2.3 텐서플로 프로토콜 버퍼"]},{"cell_type":"markdown","metadata":{"id":"esqBapNcrs2Y"},"source":["Feature은 byteslist, floatlist, int64중 하나를 담고있음.\n","\n","Features는 특성이름과 특성값을 매핑한 딕셔너리를 가짐.\n","\n","Example은 '하나'의 Feature 객체를 가짐"]},{"cell_type":"code","metadata":{"id":"MIHaIEUuvQ9j"},"source":["from tensorflow.train import BytesList, FloatList, Int64List\n","from tensorflow.train import Feature, Features, Example\n","\n","\n","#Example 프로토콜 버퍼 만들기.\n","#샘플마다 하나의 example 프로토콜 버퍼\n","person_example = Example(\n","    features=Features(\n","        feature={\n","            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n","            \"id\": Feature(int64_list=Int64List(value=[123])),\n","            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n","        }))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWFvqtxTsvzB"},"source":["#Example 프로토콜 버퍼 직렬화 해서 TFRecord 파일로 저장하기\n","with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n","    f.write(person_example.SerializeToString())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4z5w7tq8vpag"},"source":["### 13.2.3 Example 프로토콜 버퍼를 읽고 파싱하기"]},{"cell_type":"markdown","metadata":{"id":"7Ot2VWhSvvYw"},"source":["Example 프로토콜 버퍼를 읽기 위해\n","\n","다시 tf.data.TFRecordDatset을 사용하여 불러오고\n","\n","tf.io.parse_single_example(직렬화된 tfrecord, description)을 사용하여 각 example을 파싱"]},{"cell_type":"code","metadata":{"id":"g8zmFrfYw5yZ"},"source":["feature_description = {\n","    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"), #크기,타입,기본값을 표현한 FixedLenFeature\n","    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0), #FixedLenFeature\n","    \"emails\": tf.io.VarLenFeature(tf.string), #길이가 가변적인 경우 특성의 타입만 표현한 tf.io.VarLenFeature\n","}\n","for serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n","    parsed_example = tf.io.parse_single_example(serialized_example, #직렬화된 tf record와\n","                                                feature_description) #설명 필요."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7nzgJWW7xOXl"},"source":["parsed_example"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PkWRHfUxxRIb"},"source":["parsed_example[\"emails\"].values[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cqRmTmgHBl1q"},"source":["가변길이특성은 희소텐서로 파싱됨. \n","\n","tf.sparse.to_dense: 희소텐서를 밀집텐서로 변환"]},{"cell_type":"code","metadata":{"id":"HXOhLiyPxS3Z"},"source":["tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-8SwF6_xUt1"},"source":["parsed_example[\"emails\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ua08ubp7Cf34"},"source":["### 13.2.4 SequenceExample 프로토콜 버퍼를 사용해 리스트의 리스트 다루기"]},{"cell_type":"markdown","metadata":{"id":"7dTrb242Dnxn"},"source":["문서는 문장의 리스트 : 리스트의 리스트"]},{"cell_type":"code","metadata":{"id":"WTjqET4ExWXU"},"source":["from tensorflow.train import FeatureList, FeatureLists, SequenceExample\n","\n","context = Features(feature={\n","    \"author_id\": Feature(int64_list=Int64List(value=[123])),\n","    \"title\": Feature(bytes_list=BytesList(value=[b\"A\", b\"desert\", b\"place\", b\".\"])),\n","    \"pub_date\": Feature(int64_list=Int64List(value=[1623, 12, 25]))\n","})\n","\n","content = [[\"When\", \"shall\", \"we\", \"three\", \"meet\", \"again\", \"?\"],\n","           [\"In\", \"thunder\", \",\", \"lightning\", \",\", \"or\", \"in\", \"rain\", \"?\"]]\n","comments = [[\"When\", \"the\", \"hurlyburly\", \"'s\", \"done\", \".\"],\n","            [\"When\", \"the\", \"battle\", \"'s\", \"lost\", \"and\", \"won\", \".\"]]\n","\n","def words_to_feature(words):\n","    return Feature(bytes_list=BytesList(value=[word.encode(\"utf-8\")\n","                                               for word in words]))\n","\n","content_features = [words_to_feature(sentence) for sentence in content]\n","comments_features = [words_to_feature(comment) for comment in comments]\n","            \n","#하나으 Feature객체와 한개 이상의 FeatureList를 가진 FeatureLists\n","sequence_example = SequenceExample(\n","    context=context,\n","    feature_lists=FeatureLists(feature_list={\n","        \"content\": FeatureList(feature=content_features),\n","        \"comments\": FeatureList(feature=comments_features)\n","    }))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9w5o0TByFaa"},"source":["sequence_example"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DDvzQYeUD9Yb"},"source":["SequenceExample 직렬화"]},{"cell_type":"code","metadata":{"id":"HwZDA9ehyG-n"},"source":["serialized_sequence_example = sequence_example.SerializeToString()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BoMzFKlnELWY"},"source":["SequenceExample 파싱\n","\n","설명도 두개가 필요(1.Features, 2.FeatureList)"]},{"cell_type":"code","metadata":{"id":"kSNC-x5hyI5H"},"source":["#파싱을 위한 context 설명\n","context_feature_descriptions = {\n","    \"author_id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n","    \"title\": tf.io.VarLenFeature(tf.string),\n","    \"pub_date\": tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0]),\n","}\n","#파싱을 위한 FeatureList 설명\n","sequence_feature_descriptions = {\n","    \"content\": tf.io.VarLenFeature(tf.string),\n","    \"comments\": tf.io.VarLenFeature(tf.string),\n","}\n","\n","#하나의 SequenceExample 파싱: parse_single_sequence_example()\n","parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n","    serialized_sequence_example, context_feature_descriptions,\n","    sequence_feature_descriptions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kNVYzLjPyJ5a"},"source":["parsed_context"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88sHCJkXyLCW"},"source":["parsed_context[\"title\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktFBHPzqyMzn"},"source":["parsed_feature_lists"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SbgeL8OwEwAw"},"source":["가변길이 시퀀스를 갖고있는 특성리스트 래그드텐서로 바꾸기"]},{"cell_type":"code","metadata":{"id":"HqYMAG7GyOe2"},"source":["print(tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dvgo0ft4FMeK"},"source":["# 13.3 입력 특성 전처리"]},{"cell_type":"markdown","metadata":{"id":"SDDOzT-9GJsQ"},"source":["람다층"]},{"cell_type":"code","metadata":{"id":"I9ZZBrxWyPuk"},"source":["means = np.mean(X_train, axis=0, keepdims=True)\n","stds = np.std(X_train, axis=0, keepdims = True)\n","eps = keras.backend.epsilon()\n","model = keras.models.Sequential([\n","                                 #람다층\n","          keras.layers.Lambda(lambda inputs: (input-means)/(std+eps))\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MfjFRClpG5B4"},"source":["사용자 정의 층"]},{"cell_type":"code","metadata":{"id":"m9Zwq0QOGzY0"},"source":["class Standardization(keras.layers.Layer):\n","  def adapt(self,data_sample):\n","    self.means_ = np.mean(data_sample,axis=0,keepdims=True)\n","    self.stds_ = np.std(data_sample,axis=0,keepdims=True)\n","  def call(self,inputs):\n","    return (inputs-self.means_)/(self.stds_ + keras.backend.epsilon)\n","\n","#층 추가 전 adapt로 계산\n","# std_layer = Standardization()\n","# std_layer.adapt(data_sample)\n","\n","# 층 추가\n","# model= keras.Sequentail()\n","# model.add(std_layer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j7g26MARJF_0"},"source":["### 13.3.1 원-핫 벡터를 사용해 범주형 특성 인코딩하기"]},{"cell_type":"code","metadata":{"id":"z0Yn0rAUHQnN"},"source":["import os\n","import tarfile\n","import urllib.request\n","\n","DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/rickiepark/handson-ml2/master/\"\n","HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n","HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n","\n","def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n","    os.makedirs(housing_path, exist_ok=True)\n","    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n","    urllib.request.urlretrieve(housing_url, tgz_path)\n","    housing_tgz = tarfile.open(tgz_path)\n","    housing_tgz.extractall(path=housing_path)\n","    housing_tgz.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FkOCTwKIBGj"},"source":["fetch_housing_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x2CL78snJV6O"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNciS49bJlBb"},"source":["csv_path = os.path.join(HOUSING_PATH, \"housing.csv\")\n","housing = pd.read_csv(csv_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FhUboDSsJn_h"},"source":["housing.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nF-Nh_KEV0i3"},"source":["vocab = [\"<1H OCEAN\",\"INLAND\",\"NEAR OCEAN\",\"NEAR BAY\",\"ISLAND\"]\n","#어휘 사전 정의\n","indices = tf.range(len(vocab), dtype = tf.int64)\n","#인덱스 텐서\n","table_init = tf.lookup.KeyValueTensorInitializer(vocab,indices)\n","#초기화 객체\n","num_oov_buckets = 2\n","#oov 를 위한 버켓. 충분히 안주면 같은 범주로 할당돼 충돌가능\n","table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wp8xLQyRWUO0"},"source":["categories = tf.constant([\"NEAR BAY\",\"DESERT\",\"INLAND\",\"INLAND\"])\n","cat_indices = table.lookup(categories) \n","cat_indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yas_CmyaW7v0"},"source":["cat_one_hot =tf.one_hot(cat_indices, depth = len(vocab)+ num_oov_buckets)\n","cat_one_hot"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Io6ylYjNCxp"},"source":["### 13.3.2 임베딩을 사용해 범주형 특성 인코딩하기"]},{"cell_type":"markdown","metadata":{"id":"9V4_a_umXimY"},"source":["초기화"]},{"cell_type":"code","metadata":{"id":"n2vYq2f7KCA7"},"source":["embedding_dim =2\n","embed_init = tf.random.uniform([len(vocab)+ num_oov_buckets, embedding_dim])\n","embedding_matrix = tf.Variable(embed_init)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lXI260_rXhyL"},"source":["embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rsG7QEdFX3Gi"},"source":["categories = tf.constant([\"NEAR BAY\",\"DESERT\",\"INLAND\",\"INLAND\"])\n","cat_indices = table.lookup(categories)\n","cat_indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eApNHhSFYTaP"},"source":["tf.nn.embedding_lookup(embedding_matrix,cat_indices)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnrV2-m5YZtI"},"source":["embedding = keras.layers.Embedding(input_dim =len(vocab)+num_oov_buckets,\n","                                   output_dim = embedding_dim)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lMAodiMPYxn2"},"source":["embedding(cat_indices)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nd6rX8ZFhkX0"},"source":["원핫인코딩 , 임베딩을 이용하여 \n","\n"," 임베딩을 학습하는 케라스 모델"]},{"cell_type":"code","metadata":{"id":"TrIiOHFXhW0v"},"source":["regular_inputs = keras.layers.Input(shape=[8])\n","categories = keras.layers.Input(shape= [], dtype = tf.string)\n","#두개의 인풋\n","\n","cat_indices = keras.layers.Lambda(lambda cats : table.lookup(cats))(categories)\n","#인덱스 찾기\n","cat_embed = keras.layers.Embedding(input_dim= 6, output_dim= 2)(cat_indices)\n","#임베딩에서 인덱스로 찾기\n","encoded_inputs = keras.layers.concatenate([regular_inputs,cat_embed])\n","outputs =keras.layers.Dense(1)(encoded_inputs)\n","\n","model =keras.models.Model(inputs =[regular_inputs,categories],\n","                          outputs =[outputs])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bxlyODXtkYJy"},"source":["### 13.3.3 케라스 전처리 층"]},{"cell_type":"markdown","metadata":{"id":"lFBaAPwmnUt3"},"source":["keras.layers.Discretiztion : 구간을 나눠 원핫 인코딩 해줌\n","\n","keras.layers.Normalization\n","\n","keras.layers.TextVectorization: 인코딩,임베딩\n","\n","\n","**adapt() 후 일반적인 층처럼 이용 가능\n","\n","\n","PreprocessingStage 클래스를 사용해 연걸하여 파이프라인처럼 사용 가능"]},{"cell_type":"code","metadata":{"id":"XHkcvBIMkBla"},"source":["# normalization = keras.layers.Normalization()\n","# discretization =keras.layers.Discretization([...])\n","\n","# pipeline = keras.layers.PreprocessingStage([normalization,discretization])\n","\n","# pipeline.adapt(data_sampe)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LVRMNefHpQIH"},"source":["## 13.4 TF 변환"]},{"cell_type":"code","metadata":{"id":"ng3hX53uonUE"},"source":["try:\n","    import tensorflow_transform as tft\n","\n","    def preprocess(inputs):  # inputs is a batch of input features\n","        median_age = inputs[\"housing_median_age\"]\n","        ocean_proximity = inputs[\"ocean_proximity\"]\n","        standardized_age = tft.scale_to_z_score(median_age - tft.mean(median_age))\n","        ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n","        return {\n","            \"standardized_median_age\": standardized_age,\n","            \"ocean_proximity_id\": ocean_proximity_id\n","        }\n","except ImportError:\n","    print(\"TF Transform is not installed. Try running: pip3 install -U tensorflow-transform\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6gEkQNmXq2oe"},"source":["# 13.5 텐서플로 데이터셋 (TFDS) 프로젝트"]},{"cell_type":"markdown","metadata":{"id":"8gPXZEqVraLU"},"source":["다양한 표준 데이터셋"]},{"cell_type":"code","metadata":{"id":"YD2KgJTeqF9c"},"source":["import tensorflow_datasets as tfds\n","\n","datasets = tfds.load(name=\"mnist\")\n","mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lTdGvsacrp4h"},"source":["print(tfds.list_builders())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2As0cbEur3nI"},"source":["plt.figure(figsize=(6,3))\n","mnist_train = mnist_train.repeat(5).batch(32).prefetch(1)\n","for item in mnist_train:\n","    images = item[\"image\"]\n","    labels = item[\"label\"]\n","    for index in range(5):\n","        plt.subplot(1, 5, index + 1)\n","        image = images[index, ..., 0]\n","        label = labels[index].numpy()\n","        plt.imshow(image, cmap=\"binary\")\n","        plt.title(label)\n","        plt.axis(\"off\")\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hMl5XS8srAs"},"source":["dataset = tfds.load(name='mnist')\n","\n","mnist_train,mnist_test = datasets['train'], datasets['test']\n","mnist_train = mnist_train.repeat(5).batch(32)\n","\n","print(mnist_train.take(1))\n","#원래는 딕셔너리 형태\n","mnist_train =mnist_train.map(lambda items: (items[\"image\"],items[\"label\"]))\n","print(mnist_train.take(1))\n","#keras 는 (특성, 레이블) 튜플을 원하기 때문에 map으로 변환.\n","mnist_train =mnist_train.prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nPXxOj_DuJNk"},"source":["for images,labels in mnist_train.take(1):\n","  print(images.shape)\n","  print(labels.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nOpBQJejuhZs"},"source":["keras.backend.clear_session()\n","np.random.seed(42)\n","tf.random.set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hk3-Di3funK_"},"source":["datasets = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n","#as_supervised=True 지도학습을 위해 (특성,레이블) 튜플로 제공해줌\n","\n","print(datasets)\n","#test와 train의 딕셔너리 형태\n","mnist_train = datasets[\"train\"].prefetch(1)\n","\n","model = keras.models.Sequential([\n","    keras.layers.Flatten(input_shape=[28, 28, 1]),\n","    keras.layers.Lambda(lambda images: tf.cast(images, tf.float32)),\n","    keras.layers.Dense(10, activation=\"softmax\")])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jD1fiquzv1x8"},"source":["model.compile(loss = keras.losses.sparse_categorical_crossentropy, optimizer= keras.optimizers.SGD(lr=1e-3),\n","              metrics =[\"accuracy\"])\n","model.fit(mnist_train, steps_per_epoch =  60000//32, epochs=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VfnmpoBax4SJ"},"source":["# 연습문제 9"]},{"cell_type":"markdown","metadata":{"id":"9insFIB2c-gZ"},"source":["a.\n","_문제: (10장에서 소개한) 패션 MNIST 데이터셋을 적재하고 훈련 세트, 검증 세트, 테스트 세트로 나눕니다. \n","\n","훈련 세트를 섞은 다음 각 데이터셋을 TFRecord 파일로 저장합니다. \n","\n","각 레코드는 두 개의 특성을 가진 Example 프로토콜 버퍼, 즉 직렬화된 이미지(tf.io.serialize_tensor()를 사용해 이미지를 직렬화하세요)와 레이블입니다. \n","\n","참고: 용량이 큰 이미지일 경우 tf.io.encode_jpeg()를 사용할 수 있습니다. 많은 공간을 절약할 수 있지만 이미지 품질이 손해를 봅니다._"]},{"cell_type":"markdown","metadata":{"id":"svz9MRrEeGfg"},"source":["데이터 불러오기"]},{"cell_type":"code","metadata":{"id":"g6vo7g0Mwrra"},"source":["(X_train_full,y_train_full),(X_test,y_test) = keras.datasets.fashion_mnist.load_data()\n","\n","\n","X_train,X_valid = X_train_full[5000:],X_train_full[:5000]\n","y_train,y_valid = y_train_full[5000:],y_train_full[:5000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAbh6CKYd_EB"},"source":["keras.backend.clear_session()\n","np.random.seed(42)\n","tf.random.set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GblUc4JieIOX"},"source":["훈련세트 섞고 TFRecord 파일로 저장하기"]},{"cell_type":"code","metadata":{"id":"QLAIZmB7eAYE"},"source":["train_set = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n","train_set= train_set.shuffle(len(X_train))\n","\n","valid_set = tf.data.Dataset.from_tensor_slices((X_valid,y_valid))\n","valid_set = valid_set.shuffle(len(X_valid))\n","\n","test_set = tf.data.Dataset.from_tensor_slices((X_test,y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aDaA8W2ugajl"},"source":["Example 프로토콜 버퍼 생성"]},{"cell_type":"code","metadata":{"id":"cw34YD5Celhv"},"source":["def create_example(image,label):\n","  image_data = tf.io.serialize_tensor(image)\n","  return Example(\n","    features = Features(\n","      feature={\n","        \"image\":Feature(bytes_list = BytesList(value=[image_data.numpy()])),\n","        \"label\":Feature(int64_list =Int64List(value=[label]))\n","      }\n","    )\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y3v1M_UTesxt"},"source":["  for image, label in valid_set.take(1):\n","    print(create_example(image,label))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wf4RMtm3g5Cv"},"source":["다음 함수는 주어진 데이터셋을 일련의 TFRecord 파일로 저장합니다. 이 예제는 라운드-로빈 방식으로 파일에 저장합니다. 이를 위해 dataset.enumerate() 메서드로 모든 샘플을 순회하고 저장할 파일을 겨정하기 위해 index % n_shards를 계산합니다. 표준 contextlib.ExitStack 클래스를 사용해 쓰는 동안 I/O 에러의 발생 여부에 상관없이 모든 writer가 적절히 종료되었는지 확인합니다."]},{"cell_type":"code","metadata":{"id":"ZTBoDToRgHqI"},"source":["from contextlib import ExitStack\n","\n","def write_tfrecords(name, dataset, n_shards=10):\n","    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n","             for index in range(n_shards)]\n","    with ExitStack() as stack:\n","        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n","                   for path in paths]\n","        for index, (image, label) in dataset.enumerate():\n","            shard = index % n_shards\n","            example = create_example(image, label)\n","            writers[shard].write(example.SerializeToString())\n","    return paths"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfk39DK2gv9R"},"source":["train_filepaths = write_tfrecords(\"my_fashion_mnist.train\", train_set)\n","valid_filepaths = write_tfrecords(\"my_fashion_mnist.valid\", valid_set)\n","test_filepaths = write_tfrecords(\"my_fashion_mnist.test\", test_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F3h1mceWg9VK"},"source":["b.\n","문제: tf.data로 각 세트를 위한 효율적인 데이터셋을 만듭니다. \n","\n","마지막으로 이 데이터셋으로 입력 특성을 표준화하는 전처리 층을 포함한 케라스 모델을 훈련합니다. \n","\n","텐서보드로 프로파일 데이터를 시각화하여 가능한 한 입력 파이프라인을 효율적으로 만들어보세요."]},{"cell_type":"markdown","metadata":{"id":"mApM4pizjw_t"},"source":["map 위한 파싱 함수"]},{"cell_type":"code","metadata":{"id":"UenD_enNgxkZ"},"source":["def preprocess(tfrecord):\n","  feature_descriptions ={\n","        \"image\" : tf.io.FixedLenFeature([], tf.string,default_value=\"\"),\n","        \"label\" : tf.io.FixedLenFeature([],tf.int64, default_value=-1)\n","  } \n","\n","  example = tf.io.parse_single_example(tfrecord,feature_descriptions)\n","  image =tf.io.parse_tensor(example[\"image\"], out_type = tf.uint8)\n","\n","  image = tf.reshape(image,shape=[28,28])\n","  return image, example[\"label\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NBPiKuVUjz6U"},"source":["전처리 함수(데이터 적재와 전처리 합치기)\n","\n","TFRecord 포멧과 tf.data api 사용"]},{"cell_type":"code","metadata":{"id":"7sjg2tLLhrCt"},"source":["def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n","                  n_parse_threads=5, batch_size=32, cache=True):\n","  dataset = tf.data.TFRecordDataset(filepaths,\n","                                    num_parallel_reads=n_read_threads)\n","  if cache:\n","    dataset = dataset.cache()\n","  if shuffle_buffer_size:\n","    dataset =dataset.shuffle(shuffle_buffer_size)\n","  dataset =dataset.map(preprocess,num_parallel_calls=n_parse_threads)\n","  dataset = dataset.batch(batch_size)\n","\n","  return dataset.prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E-FbxpZDimd9"},"source":["train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n","valid_set = mnist_dataset(valid_filepaths)\n","test_set = mnist_dataset(test_filepaths)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GlT6qE26i7hq"},"source":["train_set"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXWjFTcvjQWK"},"source":["for X,y in train_set.take(1):\n","  for i in range(5):\n","    plt.subplot(1,5,i+1)\n","    plt.imshow(X[i].numpy(), cmap='binary')\n","    plt.axis(\"off\")\n","    plt.title(str(y[i].numpy()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"46TwFlXXk9B3"},"source":["입력 특성을 표준화하는 전처리 층\n","\n","--> 사용자 정의 층 방법 :모델에 포함하는 방법. \n"]},{"cell_type":"code","metadata":{"id":"An3JAI9FjS5W"},"source":["keras.backend.clear_session()\n","tf.random.set_seed(42)\n","np.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXyUPczulAN7"},"source":["class Standardiztion(keras.layers.Layer):\n","  def adapt(self,data_sample):\n","      self.means_ = np.mean(data_sample,axis=0, keepdims=True)\n","      self.stds_ = np.std(data_sample,axis=0,keepdims=True)\n","\n","  def call(self, inputs):\n","    return (inputs-self.means_)/(self.stds_+keras.backend.epsilon())\n","\n","standardization = Standardiztion(input_shape=[28,28])\n","\n","sample_image_batches = train_set.take(100).map(lambda image, label: image)\n","sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n","                               axis=0).astype(np.float32)\n","\n","standardization.adapt(sample_images)\n","\n","model = keras.models.Sequential([\n","                                standardization,\n","                                 keras.layers.Flatten(),\n","                                 keras.layers.Dense(100,activation='relu'),\n","                                 keras.layers.Dense(10,activation='softmax')\n","                                \n","])\n","\n","model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n","              optimizer = keras.optimizers.Nadam(),metrics=\"accuracy\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tu6_Eff9m3wm"},"source":["텐서보드를 위한 콜백, 학습"]},{"cell_type":"code","metadata":{"id":"z_W-mVeFmw-2"},"source":["from datetime import datetime\n","logs =os.path.join(os.curdir, \"my_logs\",\n","                   \"run_\"+datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n","\n","tensorboard_cb = keras.callbacks.TensorBoard(\n","    log_dir=logs, histogram_freq=1, profile_batch=10\n",")\n","\n","model.fit(train_set,epochs=5,validation_data=valid_set,\n","          callbacks=[tensorboard_cb])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ubOVZpgnXcV"},"source":["%load_ext tensorboard\n","%tensorboard --logdir=./my_logs --port=6006"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nqDMP8bcndZc"},"source":["## 연습문제 10 \n","(어려워서 옮겨놓은 후 분석)"]},{"cell_type":"markdown","metadata":{"id":"TEkRSbcknt_g"},"source":["문제: 이 연습문제에서 데이터셋을 다운로드 및 분할하고 tf.data.Dataset 객체를 만들어 데이터를 적재하고 효율적으로 전처리하겠습니다. 그다음 Embedding 층을 포함한 이진 분류 모델을 만들고 훈련시킵니다."]},{"cell_type":"markdown","metadata":{"id":"tLKvSda8ojj7"},"source":["a.\n","문제: 인터넷 영화 데이터베이스의 영화 리뷰 50,000개를 담은 영화 리뷰 데이터셋을 다운로드합니다. \n","\n","이 데이터는 train과 test라는 두 개의 디렉터리로 구성되어 있습니다. \n","\n","각 디렉터리에는 12,500개의 긍정 리뷰를 담은 pos 서브디렉터리와 12,500개의 부정 리뷰를 담은 neg 서브디렉터리가 있습니다. \n","\n","리뷰는 각각 별도의 텍스트 파일에 저장되어 있습니다. (전처리된 BOW를 포함해) 다른 파일과 디렉터리가 있지만 이 연습문제에서는 무시합니다."]},{"cell_type":"code","metadata":{"id":"DsPNJqXFnabq"},"source":["from pathlib import Path\n","\n","DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n","FILENAME = \"aclImdb_v1.tar.gz\"\n","filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\n","#압축 풀기 전 '/root/.keras/datasets/aclImdb_v1.tar.gz'\n","#압축 푼 후 /root/.keras/datasets/aclImdb\n","path = Path(filepath).parent / \"aclImdb\"\n","path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XobMV6R_n58Y"},"source":["#aclImdb 파일 구조를 시각화\n","\n","for name, subdirs, files in os.walk(path):\n","  #os.walk(path) : path의 폴더 트리구조 반환. 상위부터 하위까지.\n","    indent = len(Path(name).parts) - len(path.parts)\n","    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n","    for index, filename in enumerate(sorted(files)):\n","        if index == 3:\n","            print(\"    \" * (indent + 1) + \"...\")\n","            break\n","        print(\"    \" * (indent + 1) + filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"14ol471sn830"},"source":["def review_paths(dirpath):\n","    return [str(path) for path in dirpath.glob(\"*.txt\")]\n","    #glob : *.txt로 끝나는 것들 다 반환\n","\n","train_pos = review_paths(path / \"train\" / \"pos\") #path/train/pos 의 .txt 다 반환\n","train_neg = review_paths(path / \"train\" / \"neg\")\n","test_valid_pos = review_paths(path / \"test\" / \"pos\")\n","test_valid_neg = review_paths(path / \"test\" / \"neg\")\n","\n","len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gcHl8jW9oVSO"},"source":["b.\n","문제: 테스트 세트를 검증 세트(15,000개)와 테스트 세트(10,000개)로 나눕니다."]},{"cell_type":"code","metadata":{"id":"ZYXlSOdKoSl9"},"source":["np.random.shuffle(test_valid_pos)\n","#섞음.\n","test_pos = test_valid_pos[:5000]\n","test_neg = test_valid_neg[:5000]\n","valid_pos = test_valid_pos[5000:]\n","valid_neg = test_valid_neg[5000:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zaloPRu05jR"},"source":["test_pos[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1h-0-7T61EHQ"},"source":["with open(\"/root/.keras/datasets/aclImdb/test/pos/2502_8.txt\") as review_file:\n","        a=review_file.read()\n","print(type(a))\n","print(a)\n","#read()는 스트링 반환"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7f87FiRnpfup"},"source":["c.\n","문제: tf.data를 사용해 각 세트에 대한 효율적인 데이터셋을 만듭니다."]},{"cell_type":"code","metadata":{"id":"oeAlyZr2oaDv"},"source":["def imdb_dataset(filepaths_positive, filepaths_negative):\n","  reviews=[]\n","  labels=[]\n","\n","  for filepaths, label in ((filepaths_negative,0), (filepaths_positive,1)):#레이블 부여\n","    for filepath in filepaths:\n","      with open(filepath) as review_file:\n","        reviews.append(review_file.read())\n","        #하나씩 파일 열어서 리스트에 str을 append\n","      labels.append(label)\n","\n","  #tf.data api로 변환.\n","  return tf.data.Dataset.from_tensor_slices(\n","      (tf.constant(reviews), tf.constant(labels))\n","  )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1ZDFTXmqxVK"},"source":["#경로를 각각 받아서, 레이블 주고 tf.data api로 변경\n","for X,y in imdb_dataset(train_pos,train_neg).take(3): \n","  print(X)\n","  print(y)\n","  print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kdjWzRmfr9yo"},"source":["#tf.data api를 사용하여 데이터를 전처리.\n","batch_size = 32\n","\n","train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n","valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n","test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdX5XzHWrbQ5"},"source":["d.\n","_문제: 리뷰를 전처리하기 위해 TextVectorization 층을 사용한 이진 분류 모델을 만드세요.\n","\n"," TextVectorization 층을 아직 사용할 수 없다면 (또는 도전을 좋아한다면) 사용자 전처리 층을 만들어보세요. \n"," \n"," tf.strings 패키지에 있는 함수를 사용할 수 있습니다. 예를 들어 lower()로 소문자로 만들거나 regex_replace()로 구두점을 공백으로 바꾸고 split()로 공백을 기준으로 단어를 나눌 수 있습니다. \n"," \n"," 룩업 테이블을 사용해 단어 인덱스를 출력하세요. adapt() 메서드로 미리 층을 적응시켜야 합니다._\n","\n","\n","\n","\n","먼저 리뷰를 전처리하는 함수를 만듭니다. 이 함수는 리뷰를 300자로 자르고 소문자로 변환합니다. 그다음 br /와 글자가 아닌 모든 문자를 공백으로 바꾸고 리뷰를 단어로 분할해 마지막으로 각 리뷰가 n_words 개수의 토큰이 되도록 패딩하거나 잘라냅니다:"]},{"cell_type":"code","metadata":{"id":"sbp8ghKrrTvz"},"source":["def preprocess(X_batch, n_words=50):\n","    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n","    Z = tf.strings.substr(X_batch, 0, 300)\n","    #300자 서브스트링으로 자름\n","    Z = tf.strings.lower(Z)\n","    #소문자\n","    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n","    #br 공백으로\n","    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n","    #글자아닌거 공백으로\n","    Z = tf.strings.split(Z)\n","    #띄어쓰기 단어 스플릿\n","    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n","    #텐서로\n","\n","\n","X_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\n","preprocess(X_example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQxVHuAVCCAu"},"source":["### 사용자 정의층 말고 layers.experiments.preprocessing.TextVectorization 사용해보기"]},{"cell_type":"code","metadata":{"id":"ldRsk3yVDlb_"},"source":["print(train_set)\n","\n","#레이블 떼기 (textvectorization은 X만 필요)\n","train_example = train_set.map(lambda x,y:x)\n","\n","for i in train_example.take(1):\n","  print(i)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HBC5FtzI52UQ"},"source":["from tensorflow import keras\n","\n","\n","TextVectorization_layer = keras.layers.experimental.preprocessing.TextVectorization(output_mode= \"count\")\n","TextVectorization_layer.adapt(train_example.take(1))\n","\n","inputs = keras.layers.Input(shape=[],dtype=tf.string)\n","cat_indices =TextVectorization_layer(inputs)                        \n","layer = keras.layers.Dense(100, activation=\"relu\")(cat_indices)\n","output = keras.layers.Dense(1, activation=\"sigmoid\")(layer)\n","\n","model = keras.models.Model(inputs=[inputs],\n","                           outputs =[output])\n","    \n","\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n","              metrics=[\"accuracy\"])\n","model.fit(train_set, epochs=5, validation_data=valid_set)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OamYNPrQsX_n"},"source":[" from collections import Counter\n","\n","def get_vocabulary(data_sample, max_size=1000):\n","    preprocessed_reviews = preprocess(data_sample).numpy()\n","    counter = Counter()\n","    for words in preprocessed_reviews:\n","        for word in words:\n","            if word != b\"<pad>\":\n","                counter[word] += 1\n","    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]\n","\n","get_vocabulary(X_example)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M6FipX8Wsp2h"},"source":["class TextVectorization(keras.layers.Layer):\n","    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n","        super().__init__(dtype=dtype, **kwargs)\n","        self.max_vocabulary_size = max_vocabulary_size\n","        self.n_oov_buckets = n_oov_buckets\n","\n","    def adapt(self, data_sample):\n","        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n","        words = tf.constant(self.vocab)\n","        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n","        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n","        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n","        \n","    def call(self, inputs):\n","        preprocessed_inputs = preprocess(inputs)\n","        return self.table.lookup(preprocessed_inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GWlj5zgtsxHW"},"source":["text_vectorization = TextVectorization()\n","\n","text_vectorization.adapt(X_example)\n","text_vectorization(X_example)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HVm1CIL_syb8"},"source":["max_vocabulary_size = 1000\n","n_oov_buckets = 100\n","\n","sample_review_batches = train_set.map(lambda review, label: review)\n","sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),\n","                                axis=0)\n","\n","text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,\n","                                       input_shape=[])\n","text_vectorization.adapt(sample_reviews)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1IVdeREs1FB"},"source":["text_vectorization(X_example)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KnQCUXbps6sY"},"source":["text_vectorization.vocab[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iT1udwq3s8yJ"},"source":["simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\n","tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PHKJdzhtHQU"},"source":["class BagOfWords(keras.layers.Layer):\n","    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n","        super().__init__(dtype=tf.int32, **kwargs)\n","        self.n_tokens = n_tokens\n","    def call(self, inputs):\n","        one_hot = tf.one_hot(inputs, self.n_tokens)\n","        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgmG-juXtIb0"},"source":["bag_of_words = BagOfWords(n_tokens=4)\n","bag_of_words(simple_example)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Izt5qsuKtJ9-"},"source":["n_tokens = max_vocabulary_size + n_oov_buckets + 1 # add 1 for <pad>\n","bag_of_words = BagOfWords(n_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOdj9r1AtLQQ"},"source":["model = keras.models.Sequential([\n","                                \n","    text_vectorization,\n","    bag_of_words,\n","    keras.layers.Dense(100, activation=\"relu\"),\n","    keras.layers.Dense(1, activation=\"sigmoid\"),\n","])\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n","              metrics=[\"accuracy\"])\n","model.fit(train_set, epochs=5, validation_data=valid_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KsRn5MYetOt3"},"source":["e.\n","문제: Embedding 층을 추가하고 단어 개수의 제곱근을 곱하여 리뷰마다 평균 임베딩을 계산하세요(16장 참조). 이제 스케일이 조정된 이 평균 임베딩을 모델의 다음 부분으로 전달할 수 있습니다.\n","각 리뷰의 평균 임베딩을 계산하고 리뷰의 단어 개수의 제곱근을 곱하기 위해 간단한 함수를 정의합니다:\n"]},{"cell_type":"code","metadata":{"id":"Dcu-ia8_tMuT"},"source":["def compute_mean_embedding(inputs):\n","    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n","    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)    \n","    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n","    return tf.reduce_mean(inputs, axis=1) * sqrt_n_words\n","\n","another_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n","                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n","compute_mean_embedding(another_example)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sm8Iku8ctS2G"},"source":["tf.reduce_mean(another_example, axis=1) * tf.sqrt([[2.], [1.]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJlUZp14tURh"},"source":["embedding_size = 20\n","\n","model = keras.models.Sequential([\n","    text_vectorization,\n","    keras.layers.Embedding(input_dim=n_tokens,\n","                           output_dim=embedding_size,\n","                           mask_zero=True), # <pad> tokens => zero vectors\n","    keras.layers.Lambda(compute_mean_embedding),\n","    keras.layers.Dense(100, activation=\"relu\"),\n","    keras.layers.Dense(1, activation=\"sigmoid\"),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BkZK2uhetWmO"},"source":["model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n","model.fit(train_set, epochs=5, validation_data=valid_set)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4y3U-2cltX22"},"source":["import tensorflow_datasets as tfds\n","\n","datasets = tfds.load(name=\"imdb_reviews\")\n","train_set, test_set = datasets[\"train\"], datasets[\"test\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aE0w0Vc1tZmA"},"source":["for example in train_set.take(1):\n","    print(example[\"text\"])\n","    print(example[\"label\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aseJ3FcAtb2j"},"source":[""],"execution_count":null,"outputs":[]}]}