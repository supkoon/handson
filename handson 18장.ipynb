{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"handson 18장","private_outputs":true,"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNKeo/uk6TXb80XaM3aaEAB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"CEbnf6dx3Oq7"},"source":["# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","    !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n","    !pip install -q -U tf-agents-nightly pyvirtualdisplay gym[atari]\n","    IS_COLAB = True\n","except Exception:\n","    IS_COLAB = False\n","\n","# TensorFlow ≥2.0 is required\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.0\"\n","\n","if not tf.config.list_physical_devices('GPU'):\n","    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n","    if IS_COLAB:\n","        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","# To get smooth animations\n","import matplotlib.animation as animation\n","mpl.rc('animation', html='jshtml')\n","\n","# Where to save the figures\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"rl\"\n","IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n","os.makedirs(IMAGES_PATH, exist_ok=True)\n","\n","def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n","    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(path, format=fig_extension, dpi=resolution)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eDq_XaxAHha6"},"source":["# 18.3 OpenAI 짐"]},{"cell_type":"code","metadata":{"id":"NC8ba4PcG_D4"},"source":["import gym"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4G_-NBnsdqO0"},"source":["시각화"]},{"cell_type":"code","metadata":{"id":"No_LAMj1brX-"},"source":["def update_scene(num, frames, patch):\n","    patch.set_data(frames[num])\n","    return patch,\n","\n","def plot_animation(frames, repeat=False, interval=40):\n","    fig = plt.figure()\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","    anim = animation.FuncAnimation(\n","        fig, update_scene, fargs=(frames, patch),\n","        frames=len(frames), repeat=repeat, interval=interval)\n","    plt.close()\n","    return anim\n","def render_policy_net(model, n_max_steps=200, seed=42):\n","    frames = []\n","    env = gym.make(\"CartPole-v1\")\n","    env.seed(seed)\n","    np.random.seed(seed)\n","    obs = env.reset()\n","    for step in range(n_max_steps):\n","        frames.append(env.render(mode=\"rgb_array\"))\n","        left_proba = model.predict(obs.reshape(1, -1))\n","        action = int(np.random.rand() > left_proba)\n","        obs, reward, done, info = env.step(action)\n","        if done:\n","            break\n","    env.close()\n","    return frames"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F2VTiD_CdoJ_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kO85pNQ-HgGu"},"source":["환경목록"]},{"cell_type":"code","metadata":{"id":"FpkSfR5-HAJ6"},"source":["gym.envs.registry.all()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bva4jK_bHDeG"},"source":["env = gym.make('CartPole-v1')\n","obs = env.reset()\n","obs\n","\n","#각 관측 obs는 수평위치, 카트속도, 막대 각도, 막대의 가속도를 나타냄."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u6AXXRtjH7aZ"},"source":["try:\n","    import pyvirtualdisplay\n","    display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n","except ImportError:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gwmRd277Lvut"},"source":["이미지 렌더링하여 넘파이로 배열로 받기"]},{"cell_type":"code","metadata":{"id":"M8aK_im_Ktzw"},"source":["#render() 메서드에서 변환된 렌더링된 이미지를 넘파이 배열로 받기위해\n","#mode=\"rgb_array\" 지정\n","img = env.render(mode=\"rgb_array\")\n","img.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gmu7Da_bK3pa"},"source":["def plot_environment(env, figsize=(5,4)):\n","    plt.figure(figsize=figsize)\n","    img = env.render(mode=\"rgb_array\")\n","    plt.imshow(img)\n","    plt.axis(\"off\")\n","    return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hQq6XABK50h"},"source":["plot_environment(env)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S2y-S7ZBLsCf"},"source":["가능한 행동공간 보기"]},{"cell_type":"code","metadata":{"id":"GFkijlO9K66y"},"source":["env.action_space\n","\n","#Discrete(2) == 가능한 행동 정수 0,1 2개\n","#각각 왼쪽가속(0) 오른쪽가속(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3IvAZTHPLre4"},"source":["#obs[2]>0 으로 오른쪽으로 기울어져 있기 때문에 오른쪽으로 가속해봄.\n","\n","action=1\n","obs,reward,done,info = env.step(action)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSnynWs9MJOX"},"source":["obs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"30Ua2ZLSMKgr"},"source":["reward"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uIylPFSrMMxi"},"source":["done"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXxvw_ViMNgA"},"source":["info"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UvqqY1DPMN5S"},"source":["if done:\n","    obs = env.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iPLtu_WUM6sp"},"source":["env.seed(42)\n","\n","def basic_policy(obs):\n","  angle = obs[2]\n","  return 0 if angle<0 else 1\n","\n","totals = []\n","\n","for episode in range(500):\n","  episode_rewards=0\n","  #새로운 관찰\n","  obs = env.reset()\n","  for step in range(200):\n","    #정책에 따른 행동\n","    action = basic_policy(obs)\n","    obs,reward,done,info = env.step(action)\n","    episode_rewards +=reward\n","    #done==1 <-- 넘어졌거나 200번넘음.\n","    if done:\n","      break\n","  totals.append(episode_rewards)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YuRn-c0hNS5n"},"source":["import numpy as np\n","np.mean(totals),np.std(totals),np.min(totals),np.max(totals)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pvu0OeO6O2uP"},"source":["# 18.4 신경망 정책"]},{"cell_type":"markdown","metadata":{"id":"8yj9KLA7PdRl"},"source":["신경망을 사용해 관측(obs)을 바탕으로 행동(action) 출력\n","\n","cartpole의 경우엔 'p' or '1-p'의 출력\n"]},{"cell_type":"code","metadata":{"id":"d0x2gbfYOy_R"},"source":["import tensorflow as tf\n","from tensorflow import keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZjAWCSmMPxtw"},"source":["n_inputs = env.observation_space.shape[0]\n","\n","model = keras.models.Sequential([\n","                                 keras.layers.Dense(5,activation=\"elu\", input_shape=[n_inputs]),\n","                                 keras.layers.Dense(1,activation=\"sigmoid\")#p , 1-p\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpiPWRi5QU2q"},"source":["# 18.5 행동 평가"]},{"cell_type":"markdown","metadata":{"id":"3gl3l6SARwcu"},"source":["신용할당문제로 현재에서 멀어질수록 보상에 감마를 곱하여 다 더함.\n","\n","많은 에피소드를 실행해 행동이익을 정규화 (평균, 표준편차)\n","\n","--> 행동이익 음수는 나쁨\n","\n","--> 행동이익 양수는 좋음\n"]},{"cell_type":"markdown","metadata":{"id":"s_WbHIsuSpej"},"source":["# 18.6 정책 그레디언트"]},{"cell_type":"markdown","metadata":{"id":"ftsvzNNbZrwY"},"source":["스탭 1번 함수"]},{"cell_type":"code","metadata":{"id":"kVZXZZ8GRv_U"},"source":["def play_one_step(env, obs, model, loss_fn):\n","  \n","    with tf.GradientTape() as tape:\n","        #하나의 관측과 함께 모델 호출\n","        #왼쪽으로 이동할 확률 하나 출력\n","        left_proba = model(obs[np.newaxis])\n","        \n","        #0~1사이의 랜덤한 실수를 샘플링\n","        # left_proba보다 크면 action=0, 아니면 1. --> 왼쪽0 오른쪽1으로 행동 정해짐\n","        action = (tf.random.uniform([1, 1]) > left_proba)\n","        \n","        #왼쪽으로 이동할 타깃확률 정의. 1-행동. ex)왼쪽으로 가면 1-0=1\n","        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n","\n","        #손실함수를 사용해 손실을 계산\n","        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n","    #훈련가능 변수에 대한 손실의 그레디언트 계산.    \n","    \n","    grads = tape.gradient(loss, model.trainable_variables)\n","    #선택한 행동을 플레이.==> [새로운 관측, 보상, 에피소드종료여부, 계산한 그래디언트] 반환\n","    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n","    return obs, reward, done, grads"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKhrveVTZwg-"},"source":["스탭함수를 이용해 여러 에피소드 플레이"]},{"cell_type":"code","metadata":{"id":"YYps7Fy-P8yp"},"source":["def play_multiple_episodes(env, n_episodes,n_max_steps, model,loss_fn):\n","  all_rewards=[]\n","  all_grads=[]\n","  for episode in range(n_episodes):\n","    current_rewards =[]\n","    current_grads=[]\n","    obs = env.reset()\n","    for step in range(n_max_steps):\n","      obs,reward,done,grads = play_one_step(env,obs,model,loss_fn)\n","      current_rewards.append(reward)\n","      current_grads.append(grads)\n","      if done:\n","        break\n","    all_rewards.append(current_rewards)\n","    all_grads.append(current_grads)\n","  return all_rewards, all_grads"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A4u_FnA7ap4B"},"source":["할인계수를 적용한 보상"]},{"cell_type":"code","metadata":{"id":"BQq5O6A9agTn"},"source":["def discount_rewards(rewards, discount_rate):\n","    discounted = np.array(rewards)\n","    for step in range(len(rewards) - 2, -1, -1):\n","        discounted[step] += discounted[step + 1] * discount_rate\n","    return discounted"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Shjhdo28au8F"},"source":["할인적용한 보상 정규화"]},{"cell_type":"code","metadata":{"id":"eVL2pSSNauWP"},"source":["def discount_and_normalize_rewards(all_rewards, discount_rate):\n","    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n","                              for rewards in all_rewards]\n","    flat_rewards = np.concatenate(all_discounted_rewards)\n","    reward_mean = flat_rewards.mean()\n","    reward_std = flat_rewards.std()\n","    return [(discounted_rewards - reward_mean) / reward_std\n","            for discounted_rewards in all_discounted_rewards]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2vNn13Tma1Sp"},"source":["discount_rewards([10,0,-50], discount_rate=0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nHC7hFsobCiO"},"source":["discount_and_normalize_rewards([[10,0,-50],[10,20]],discount_rate= 0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jMzYjuPbO6_"},"source":["n_iterations = 150\n","n_episodes_per_update = 10\n","n_max_steps = 200\n","discount_rate = 0.95"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ny_z76UubS85"},"source":["optimizer = keras.optimizers.Adam(lr=0.01)\n","loss_fn = keras.losses.binary_crossentropy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AKimSTdbUqT"},"source":["np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","model = keras.models.Sequential([\n","    keras.layers.Dense(5, activation=\"elu\", input_shape=[4]),\n","    keras.layers.Dense(1, activation=\"sigmoid\"),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GnYy9Z5bW4G"},"source":["env = gym.make(\"CartPole-v1\")\n","env.seed(42);\n","\n","for iteration in range(n_iterations):\n","    all_rewards, all_grads = play_multiple_episodes(\n","        env, n_episodes_per_update, n_max_steps, model)\n","    total_rewards = sum(map(sum, all_rewards))                     # Not shown in the book\n","    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          # Not shown\n","        iteration, total_rewards / n_episodes_per_update), end=\"\") # Not shown\n","    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n","                                                       discount_rate)\n","    all_mean_grads = []\n","    for var_index in range(len(model.trainable_variables)):\n","        mean_grads = tf.reduce_mean(\n","            [final_reward * all_grads[episode_index][step][var_index]\n","             for episode_index, final_rewards in enumerate(all_final_rewards)\n","                 for step, final_reward in enumerate(final_rewards)], axis=0)\n","        all_mean_grads.append(mean_grads)\n","    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n","\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ywh6giO0bYmo"},"source":["frames = render_policy_net(model)\n","plot_animation(frames)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eyNQ7Lu0L1Q7"},"source":["# 18.7 마르코프 결정과정"]},{"cell_type":"code","metadata":{"id":"I6YWcn5AcKNR"},"source":["transition_probabilities = [ # shape=[s, a, s']\n","        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n","        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n","        [None, [0.8, 0.1, 0.1], None]]\n","rewards = [ # shape=[s, a, s']\n","        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n","        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n","        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n","possible_actions = [[0, 1, 2], [0, 2], [1]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n1d3U4T7MIC_"},"source":["Q_value =np.full((3,3),-np.inf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ci1YbIT8MXPZ"},"source":["Q_value"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OevJOYgOMX60"},"source":["for state, actions in enumerate(possible_actions):\n","  Q_value[state][actions]=0.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C15xI6SBMmc-"},"source":["Q_value"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"62vFzdJDPx5z"},"source":["gamma = 0.90\n","\n","for iteration in range(50):\n","  Q_prev = Q_value.copy()\n","  for s in range(3):\n","    for a in possible_actions[s]:\n","      Q_value[s,a]=np.sum([\n","      transition_probabilities[s][a][sp]*(rewards[s][a][sp]+ gamma * \n","                                          np.max(Q_prev[sp]))\n","      for sp in range(3)\n","      ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v5X_eO6fMncJ"},"source":["Q_value"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6v7728ZtQww0"},"source":["각 상태에 대해 가장 높은 Q value를 갖는 행동 a 찾기\n","Q(s,a)"]},{"cell_type":"code","metadata":{"id":"K0I1zd7XQsD6"},"source":["np.argmax(Q_value,axis=1)\n","\n","#array([0, 0, 1])\n","#상태 0에서는 행동a0\n","#상태 1에서는 행동a0\n","#상태 2에서는 행동a1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_-lwkglbWupz"},"source":["# 18.9 Q러닝"]},{"cell_type":"markdown","metadata":{"id":"4qWZ5G0ymMNs"},"source":["http://blog.quantylab.com/rl.html\n","\n","상태 가치함수와 상태-행동 가치함수의 차이\n","\n","(가치반복, q-가치반복 차이)"]},{"cell_type":"markdown","metadata":{"id":"eiA_CD6oXLnk"},"source":["에이전트가 행동을 하나 실행하고 결과상태와 보상을 받는 스텝함수"]},{"cell_type":"code","metadata":{"id":"73wLdPkbQ7GJ"},"source":["def step(step,action):\n","  probas = transition_probabilities[state][action]\n","  next_state = np.random.choice([0,1,2],p=probas)\n","  reward  = rewards[state][action][next_state]\n","  return next_state, reward"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WSklyIKiXI9y"},"source":["랜덤한 정책"]},{"cell_type":"code","metadata":{"id":"pIoAPkLEXHtv"},"source":["def exploration_policy(state):\n","  return np.random.choice(possible_actions[state])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lnJFfWUYcJFd"},"source":["np.random.seed(42)\n","\n","Q_values = np.full((3, 3), -np.inf)\n","for state, actions in enumerate(possible_actions):\n","    Q_values[state][actions] = 0\n","\n","alpha0 = 0.05 # initial learning rate\n","decay = 0.005 # learning rate decay\n","gamma = 0.90 # discount factor\n","state = 0 # initial state\n","history2 = [] # Not shown in the book\n","\n","for iteration in range(10000):\n","    history2.append(Q_values.copy()) # Not shown\n","    action = exploration_policy(state)\n","    next_state, reward = step(state, action)\n","    next_value = np.max(Q_values[next_state]) # greedy policy at the next step\n","    alpha = alpha0 / (1 + iteration * decay)\n","    Q_values[state, action] *= 1 - alpha\n","    Q_values[state, action] += alpha * (reward + gamma * next_value)\n","    state = next_state"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5p9KH5B8cP8o"},"source":["Q_values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l7T8DR-ycTXm"},"source":["최적의 행동"]},{"cell_type":"code","metadata":{"id":"rdrBIs_EcRWS"},"source":["np.argmax(Q_values,axis=1)\n","\n","#array([0, 0, 1])\n","#s0에서는 행동0\n","#s1에서는 행동0\n","#s2에서는 행동1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aSXtYfmeyXCh"},"source":["## 18.10 심층 Q-러닝 구현하기"]},{"cell_type":"markdown","metadata":{"id":"WveUW8c7ydUy"},"source":["DQN 구현"]},{"cell_type":"code","metadata":{"id":"D8fzmRblcV5u"},"source":["keras.backend.clear_session()\n","tf.random.set_seed(42)\n","np.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4UhTsWtCyhNd"},"source":["env = gym.make(\"CartPole-v1\")\n","input_shape=[4]\n","n_outputs=2\n","\n","\n","model = keras.models.Sequential([\n","                                 keras.layers.Dense(32,activation=\"elu\",\n","                                                    input_shape=input_shape),\n","                                 keras.layers.Dense(32,activation=\"elu\"),\n","                                 keras.layers.Dense(n_outputs)\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aqVsSsElzAbi"},"source":["입실론 그리디 구현\n","\n","입실론 확률로 랜덤한 행동,\n","1-입실론 확률로 q-value를 가장 크게하는 행동"]},{"cell_type":"code","metadata":{"id":"Zs2cX6XqzDUz"},"source":["def epsilon_greedy_policy(state, epsilon=0):\n","  if np.random.rand() < epsilon:\n","    return np.random.randint(n_outputs)\n","  else:\n","    Q_values = model.predict(state[np.newaxis])\n","    return np.argmax(Q_values[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FEGrRXGuzbtV"},"source":["from collections import deque\n","\n","replay_memory = deque(maxlen=2000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ji60IQuGzdNk"},"source":["def sample_experiences(batch_size):\n","    indices = np.random.randint(len(replay_memory), size=batch_size)\n","    batch = [replay_memory[index] for index in indices]\n","    states, actions, rewards, next_states, dones = [\n","        np.array([experience[field_index] for experience in batch])\n","        for field_index in range(5)]\n","    return states, actions, rewards, next_states, dones"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUnC8nNR0DQ2"},"source":["def play_one_step(env, state, epsilon):\n","    action = epsilon_greedy_policy(state, epsilon)\n","    next_state, reward, done, info = env.step(action)\n","    replay_memory.append((state, action, reward, next_state, done))\n","    return next_state, reward, done, info"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQiYCAKM0GiZ"},"source":["batch_size = 32\n","discount_rate = 0.95\n","optimizer = keras.optimizers.Adam(lr=1e-3)\n","loss_fn = keras.losses.mean_squared_error\n","\n","def training_step(batch_size):\n","    experiences = sample_experiences(batch_size)\n","    states, actions, rewards, next_states, dones = experiences\n","    next_Q_values = model.predict(next_states)\n","    max_next_Q_values = np.max(next_Q_values, axis=1)\n","    target_Q_values = (rewards +\n","                       (1 - dones) * discount_rate * max_next_Q_values)\n","    target_Q_values = target_Q_values.reshape(-1, 1)\n","    mask = tf.one_hot(actions, n_outputs)\n","    with tf.GradientTape() as tape:\n","        all_Q_values = model(states)\n","        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n","        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n","    grads = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cc7iY5Mk0PMx"},"source":["env.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","rewards = [] \n","best_score = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJ9-KEwi0REs"},"source":["for episode in range(600):\n","    obs = env.reset()    \n","    for step in range(200):\n","        epsilon = max(1 - episode / 500, 0.01)\n","        obs, reward, done, info = play_one_step(env, obs, epsilon)\n","        if done:\n","            break\n","    rewards.append(step) # Not shown in the book\n","    if step > best_score: # Not shown\n","        best_weights = model.get_weights() # Not shown\n","        best_score = step # Not shown\n","    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\") # Not shown\n","    if episode > 50:\n","        training_step(batch_size)\n","\n","model.set_weights(best_weights)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9F0xKBr0XpG"},"source":["plt.figure(figsize=(8, 4))\n","plt.plot(rewards)\n","plt.xlabel(\"Episode\", fontsize=14)\n","plt.ylabel(\"Sum of rewards\", fontsize=14)\n","save_fig(\"dqn_rewards_plot\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aAKLvJvy4Y7M"},"source":["# 18.11 심층 Q-러닝 변종"]},{"cell_type":"markdown","metadata":{"id":"iim0M8014dHF"},"source":["### 18.11.1 고정 Q-가치 타깃"]},{"cell_type":"markdown","metadata":{"id":"w5C0Iwpmt-1f"},"source":["꼬리가 머리를 쫒는 상황이 발생하는것을 방지.\n","\n","훈련은 온라이모델.\n","\n","타겟모델은 일정 에피소드마다 따로 업데이트"]},{"cell_type":"code","metadata":{"id":"CQci6uji2UIc"},"source":["target = keras.models.clone_model(model)\n","target.set_weights(model.get_weights())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YHvFhARCu5eT"},"source":["훈련시 다음과 같이 일정 에피소드마다 가중치 복사"]},{"cell_type":"code","metadata":{"id":"R9-wznGouliM"},"source":["# next_Q_values = target.predict(next_states)\n","# if episode % 50 ==0:\n","#   target.set_weights(model.get_weights())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aiok-5Ypu_pG"},"source":["### 18.11.2 더블 DQN"]},{"cell_type":"markdown","metadata":{"id":"LzHLa-FIwQUS"},"source":["최선의 행동은 온라인모델에서 선택,\n","\n","최선 행동에 대한 Q가치는 타겟모델에서 선택."]},{"cell_type":"code","metadata":{"id":"ld0x_Ow-vB5L"},"source":["def training_step(batch_size):\n","    experiences = sample_experiences(batch_size)\n","    states, actions, rewards, next_states, dones = experiences\n","    next_Q_values = model.predict(next_states)\n","    best_next_actions = np.argmax(next_Q_values, axis=1)\n","    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n","    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n","    target_Q_values = (rewards + \n","                       (1 - dones) * discount_rate * next_best_Q_values)\n","    target_Q_values = target_Q_values.reshape(-1, 1)\n","    mask = tf.one_hot(actions, n_outputs)\n","    with tf.GradientTape() as tape:\n","        all_Q_values = model(states)\n","        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n","        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n","    grads = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X6hDbx1JzUDC"},"source":["### 18.11.4 듀얼링 DQN"]},{"cell_type":"markdown","metadata":{"id":"aTbMj8UQzXcT"},"source":["Q-value = V(s) + A(s,a) 로 표현\n","\n","A는 상태 s에서 최선의 행동을하여 다른행동 a를 했을떄보다 얻는 이득\n","\n","V와 A(s,a)를 모두 추정"]},{"cell_type":"code","metadata":{"id":"JrEJxYgKwPcz"},"source":["keras.backend.clear_session()\n","tf.random.set_seed(42)\n","np.random.seed(42)\n","n_outputs=2\n","\n","K= keras.backend\n","input_states = keras.layers.Input(shape=[4])\n","hidden1 = keras.layers.Dense(32,activation=\"elu\")(input_states)\n","hidden2 = keras.layers.Dense(32,activation=\"elu\")(hidden1)\n","state_values = keras.layers.Dense(1)(hidden2)\n","#모든 이익 추정\n","raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n","#이익 최대치를 빼서 최선 행동의 경우 0이 되도록.\n","#--> 최선행동을 하면 최선행동을 하여 다른 행동에 비해 얻는 상대적 이익이 0 이기떄문.\n","advantages = raw_advantages-K.max(raw_advantages,axis=1,keepdims=True)\n","Q_values = state_values +advantages\n","\n","model = keras.Model(inputs=[input_states], outputs = [Q_values])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YNNslvdq09S4"},"source":["# 18.12 TF-Agents 라이브러리"]},{"cell_type":"code","metadata":{"id":"QUBU-Y_Q2ZPy"},"source":["pip install -U 'gym[atari]'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lle6Ix4k4G1m"},"source":["# pip install tensorflow-probability"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJpjyUGP4xVo"},"source":["# pip install tf_agents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WmJrSfFs0bhF"},"source":["from tf_agents.environments import suite_gym\n","\n","env = suite_gym.load(\"Breakout-v4\")\n","env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDeHy5ks2PSR"},"source":["env.gym"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqfthYKZ5WKV"},"source":["env.seed(42)\n","env.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nEI1aBTA6W0U"},"source":["step\n","\n","timestep 반환"]},{"cell_type":"code","metadata":{"id":"Zm9ny8SB5Xe8"},"source":["env.step(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmQmSA5J5ajT"},"source":["img = env.render(mode = \"rgb_array\")\n","\n","plt.figure(figsize=(6,8))\n","plt.imshow(img)\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xyJwvMXD5jcp"},"source":["env.current_time_step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F6Xaf7Sd6vff"},"source":["### 18.12.3 환경스팩\n","\n","관측,행동,타임스텝크기, 데이터타입, 이름과 최솟값,최댓값을 포함하는 스펙 제공"]},{"cell_type":"markdown","metadata":{"id":"yYxr4oxW7Bpl"},"source":["관측스펙."]},{"cell_type":"code","metadata":{"id":"RVtyEE9G5mqB"},"source":["env.observation_spec()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ee6HpiWO7DgN"},"source":["행동스펙"]},{"cell_type":"code","metadata":{"id":"0kiw5rUF65ym"},"source":["env.action_spec()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3naM7G4w7JkB"},"source":["타임스텝스펙"]},{"cell_type":"code","metadata":{"id":"PbHx-IEj6-yX"},"source":["env.time_step_spec()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VNpdzLqW7YXd"},"source":["각 행동이 무엇인지"]},{"cell_type":"code","metadata":{"id":"tlgHACQW7Izl"},"source":["env.gym.get_action_meanings()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j1aw36Ax7fST"},"source":["### 18.12.4 환경래퍼와 아타리 전처리"]},{"cell_type":"markdown","metadata":{"id":"neA8nCgY71sE"},"source":["tf_agents.environments.wrappers 패키지에 여러가지 환경래퍼 제공."]},{"cell_type":"code","metadata":{"id":"TlLoAyw17R0E"},"source":["import tf_agents.environments.wrappers\n","\n","for name in dir(tf_agents.environments.wrappers):\n","    obj = getattr(tf_agents.environments.wrappers, name)\n","    if hasattr(obj, \"__base__\") and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n","        print(\"{:27s} {}\".format(name, obj.__doc__.split(\"\\n\")[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gulWTMMQ99Iv"},"source":[""]},{"cell_type":"code","metadata":{"id":"Nj1AU44S8Bbf"},"source":["from tf_agents.environments.wrappers import ActionRepeat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_q8-iVHp-PeS"},"source":["모든 행동을 4번씩 반복하게 해주는 래퍼"]},{"cell_type":"code","metadata":{"id":"87xnNFNG-Dy1"},"source":["repeating_env = ActionRepeat(env, times=4)\n","repeating_env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jz-sdVOX-OMY"},"source":["from functools import partial\n","from gym.wrappers import TimeLimit\n","\n","limited_repeating_env = suite_gym.load(\n","    \"Breakout-v4\",\n","    gym_env_wrappers=[partial(TimeLimit, max_episode_steps=10000)],\n","    env_wrappers=[partial(ActionRepeat, times=4)],\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AC_rmBwC_JYa"},"source":["limited_repeating_env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgpAv2ub_KbN"},"source":["limited_repeating_env.unwrapped"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ITed3Ze_LtQ"},"source":["from tf_agents.environments import suite_atari\n","from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n","from tf_agents.environments.atari_wrappers import FrameStack4\n","\n","max_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames\n","environment_name = \"BreakoutNoFrameskip-v4\"\n","\n","env = suite_atari.load(\n","    environment_name,\n","    max_episode_steps=max_episode_steps,\n","    gym_env_wrappers=[AtariPreprocessing, FrameStack4])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LRN384vx_4Z6"},"source":["env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GWje963x_6wB"},"source":["env.seed(42)\n","env.reset()\n","time_step = env.step(1) # FIRE\n","for _ in range(4):\n","    time_step = env.step(3) # LEFT"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7YXKMnia__bQ"},"source":["def plot_observation(obs):\n","    # Since there are only 3 color channels, you cannot display 4 frames\n","    # with one primary color per frame. So this code computes the delta between\n","    # the current frame and the mean of the other frames, and it adds this delta\n","    # to the red and blue channels to get a pink color for the current frame.\n","    obs = obs.astype(np.float32)\n","    img = obs[..., :3]\n","    current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n","    img[..., 0] += current_frame_delta\n","    img[..., 2] += current_frame_delta\n","    img = np.clip(img / 150, 0, 1)\n","    plt.imshow(img)\n","    plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJipB-JmAAb7"},"source":["plt.figure(figsize=(6, 6))\n","plot_observation(time_step.observation)\n","save_fig(\"preprocessed_breakout_plot\")\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x8UNDcYpABgV"},"source":["from tf_agents.environments.tf_py_environment import TFPyEnvironment\n","\n","tf_env = TFPyEnvironment(env)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WiSe6-PAF58"},"source":["tf.random.uniform(xv)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mzrterEZSduS"},"source":["# 연습문제 8. 정책 그레디언트를 사용해 LunarLander -v2 환경을 해결해보기"]},{"cell_type":"code","metadata":{"id":"k4nwVfyTS4FD"},"source":["pip install gym[box2d]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRTPayh3TIFt"},"source":["env = gym.make(\"LunarLander-v2\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGjOjCD1TIXO"},"source":["env.observation_space.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6LAcJAJgUXIH"},"source":["env.action_space.n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6niB5_dXtkO9"},"source":["env.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLXXkADDSjTI"},"source":["keras.backend.clear_session()\n","tf.random.set_seed(42)\n","np.random.seed(42)\n","\n","n_inputs = 8\n","\n","model = keras.models.Sequential([\n","    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n","    keras.layers.Dense(4, activation=\"softmax\"),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"66BMq_0UTPUb"},"source":["def play_one_step(env, obs, model):\n","    with tf.GradientTape() as tape:\n","        prob = model(obs[np.newaxis])\n","        # print(prob)\n","        p=prob.numpy()\n","        p/=p.sum()\n","        # print(p)\n","        action = np.random.choice(range(4), p=p[0])\n","        #액션을 하나 고름.\n","        # print(action)\n","        y_target=[[0,0,0,0]]\n","        y_target[0][action]=1\n","        y_target = tf.cast(y_target,tf.float32)\n","        # print(y_target) \n","        loss = tf.reduce_mean(keras.losses.categorical_crossentropy(y_target, prob))\n","        # print(loss)\n","    grads = tape.gradient(loss, model.trainable_variables)\n","    # print(grads)\n","    obs, reward, done, info = env.step(action)\n","    return obs, reward, done, grads\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iJDIJTsYTbxc"},"source":["def play_multiple_episodes(env, n_episodes, n_max_steps, model):\n","    all_rewards = []\n","    all_grads = []\n","    for episode in range(n_episodes):\n","        current_rewards = []\n","        current_grads = []\n","        obs = env.reset()\n","        for step in range(n_max_steps):\n","            obs, reward, done, grads = play_one_step(env, obs, model)\n","            current_rewards.append(reward)\n","            current_grads.append(grads)\n","            if done:\n","                break\n","        all_rewards.append(current_rewards)\n","        all_grads.append(current_grads)\n","    return all_rewards, all_grads"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_lKzCbpuTdQI"},"source":["def discount_rewards(rewards, discount_rate):\n","    discounted = np.array(rewards)\n","    for step in range(len(rewards) - 2, -1, -1):\n","        discounted[step] += discounted[step + 1] * discount_rate\n","    return discounted\n","\n","def discount_and_normalize_rewards(all_rewards, discount_rate):\n","    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n","                              for rewards in all_rewards]\n","    flat_rewards = np.concatenate(all_discounted_rewards)\n","    reward_mean = flat_rewards.mean()\n","    reward_std = flat_rewards.std()\n","    return [(discounted_rewards - reward_mean) / reward_std\n","            for discounted_rewards in all_discounted_rewards]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rLri3Ta2Tek5"},"source":["n_iterations = 150\n","n_episodes_per_update = 10\n","n_max_steps = 100\n","discount_rate = 1.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hlkm0Q83Ti1b"},"source":["optimizer = keras.optimizers.Adam(lr=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Owi3mXwBKvAa"},"source":["model.save('saved_model/my_model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AvG-T0O3TkXe"},"source":["for iteration in range(n_iterations):\n","    all_rewards, all_grads = play_multiple_episodes(\n","        env, n_episodes_per_update, n_max_steps, model)\n","    total_rewards = sum(map(sum, all_rewards))                     # Not shown in the book\n","    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          # Not shown\n","        iteration, total_rewards / n_episodes_per_update), end=\"\") # Not shown\n","    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n","                                                       discount_rate)\n","    all_mean_grads = []\n","    for var_index in range(len(model.trainable_variables)):\n","        mean_grads = tf.reduce_mean(\n","            [final_reward * all_grads[episode_index][step][var_index]\n","             for episode_index, final_rewards in enumerate(all_final_rewards)\n","                 for step, final_reward in enumerate(final_rewards)], axis=0)\n","        all_mean_grads.append(mean_grads)\n","    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n","\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i8zKkH22N2KU"},"source":["def update_scene(num, frames, patch):\n","    patch.set_data(frames[num])\n","    return patch,\n","\n","def plot_animation(frames, repeat=False, interval=40):\n","    fig = plt.figure()\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","    anim = animation.FuncAnimation(\n","        fig, update_scene, fargs=(frames, patch),\n","        frames=len(frames), repeat=repeat, interval=interval)\n","    plt.close()\n","    return anim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V1ufpLQhGHNU"},"source":["def render_policy_net(model, n_max_steps=200, seed=30):\n","    frames = []\n","    env = gym.make(\"LunarLander-v2\")\n","    env.seed(seed)\n","    np.random.seed(seed)\n","    obs = env.reset()\n","    for step in range(n_max_steps):\n","        frames.append(env.render(mode=\"rgb_array\"))\n","        prob = model.predict(obs.reshape(1, -1))\n","        p=prob\n","        p/=p.sum()\n","        action = np.random.choice(range(4), p=p[0])\n","        obs, reward, done, info = env.step(action)\n","        if done:\n","            break\n","    env.close()\n","    return frames"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9RkltTcMVek"},"source":["!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet\n","\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfTXGJfVTtSN"},"source":["frames = render_policy_net(model,n_max_steps=200,seed=30)\n","plot_animation(frames)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZOTdu0IFUkGX"},"source":[""],"execution_count":null,"outputs":[]}]}